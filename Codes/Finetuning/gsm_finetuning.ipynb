{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77b116e-a50f-4322-8395-2b4e940dcdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/compat/_optional.py:161: UserWarning: Pandas requires version '2.7.1' or newer of 'numexpr' (version '2.7.0' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/opt/conda/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.996-ko-0.9.2 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.996-ko-0.9.2 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn import functional as F\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import time\n",
    "from string import punctuation\n",
    "import string\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e66546-58d4-4e52-b6ea-3b288a0381b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean_and_format(df):\n",
    "    df_question = pd.DataFrame(df.question.str.split('.').str[-1])\n",
    "    df_answer = pd.DataFrame(df.answer)\n",
    "    df_context = pd.DataFrame(df.question.str.split('.').str[:-1])\n",
    "    df_context['context'] = [','.join(map(str, l)) for l in df_context['question']]\n",
    "    df_context = df_context.drop('question', axis=1)\n",
    "    # Concatenate the dataframes horizontally (axis=1)\n",
    "    final_df = pd.concat([df_context, df_question, df_answer], axis=1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b68ceb-b3fa-44b0-9010-4448ed90c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(\"train.jsonl\", lines=True)\n",
    "testing_set = pd.read_json(\"test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a12f87-91f8-44fe-a177-3a677290c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_clean_and_format(training_set)\n",
    "test_data = data_clean_and_format(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ee2288-e805-4e7f-bbe9-60b6e3d29a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define a function to remove values enclosed within << >>\\ndef remove_values(text):\\n    return re.sub('<<.*?>>', '', text)\\n\\n# Apply the function to the 'answer' column of the DataFrame\\ntrain_data['answer'] = train_data['answer'].apply(remove_values)\\ntest_data['answer'] = test_data['answer'].apply(remove_values)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define a function to remove values enclosed within << >>\n",
    "def remove_values(text):\n",
    "    return re.sub('<<.*?>>', '', text)\n",
    "\n",
    "# Apply the function to the 'answer' column of the DataFrame\n",
    "train_data['answer'] = train_data['answer'].apply(remove_values)\n",
    "test_data['answer'] = test_data['answer'].apply(remove_values)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797edae6-e1e7-4b3e-9de4-af52c9d4ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (5978, 3)\n",
      "Validation set shape: (1495, 3)\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Define the proportion of data to be used for validation\n",
    "validation_ratio = 0.20\n",
    "\n",
    "# Randomly sample the DataFrame to create the validation set\n",
    "val_data = train_data.sample(frac=validation_ratio, random_state=random_seed)\n",
    "\n",
    "# Create the train set by excluding the validation set\n",
    "train_data = train_data.drop(val_data.index)\n",
    "\n",
    "# Print the shapes of train and validation sets\n",
    "print(\"Train set shape:\", train_data.shape)\n",
    "print(\"Validation set shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708a9b8b-2d19-41b7-837f-2e525ba545fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5978, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332d1653-6238-479c-9496-c2791e47fae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5978, 3) (1495, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d7165f-5e66-4283-914d-aea44ad8ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_path = 'checkpoints/T5_span_loss_large_v2'\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained_model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af3ccc8-b828-40b9-a1aa-a754d75c9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a233b9-c7d4-4d57-9ce0-1af482c62e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSMData(Dataset):\n",
    "    def __init__(self, df, tokenizer, input_max_length=512, output_max_length=512):\n",
    "        self.dataset = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_length = input_max_length\n",
    "        self.output_length = output_max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_row = self.dataset.iloc[index]\n",
    "        source_encoding = tokenizer(\n",
    "            data_row[\"question\"],\n",
    "            data_row[\"context\"],\n",
    "            max_length=self.input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        target_encoding = tokenizer(\n",
    "            data_row[\"answer\"],\n",
    "            max_length=self.output_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        labels = target_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return dict(\n",
    "            question=data_row[\"question\"],\n",
    "            context=data_row[\"context\"],\n",
    "            answer=data_row[\"answer\"],\n",
    "            input_ids=source_encoding[\"input_ids\"].flatten(),\n",
    "            attention_mask=source_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten()\n",
    "        )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "681ae334-5030-403f-867d-469613e5c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSMDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_file, val_file, test_file, tokenizer_name_or_path, input_length=512, output_length=512, batch_size=2, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.train_file = train_file\n",
    "        self.val_file = val_file\n",
    "        self.test_file = test_file\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "             \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_data = GSMData(self.train_file, self.tokenizer)\n",
    "            self.val_data = GSMData(self.val_file, self.tokenizer)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_data = GSMData(self.test_file, self.tokenizer)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "     \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b6d4add-6983-410a-98b4-7041b6b84d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = GSMDataModule(train_data, val_data, test_data, pretrained_model_path)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2b3496-a2aa-4f54-adf0-49d021945ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSMModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(pretrained_model_path, return_dict = True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e51be1c-2575-43a1-ba20-206ddedc2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GSMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a53294b0-e476-428a-ba0f-a2469d1ff1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"Finetuned_checkpoint\",\n",
    "    filename='best_model_span_large',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e4cf4d-e46a-4eef-857e-d72d59808917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logger instance\n",
    "logger = TensorBoardLogger(save_dir='logs/', name='GSM_Finetuning_Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16e15c01-ff02-4df0-bca7-89320b2145df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs = 25,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "129d305e-77bd-463f-92ad-0715348a2d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/work/GSM8K/CustomT5/Finetuned_checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 783 M \n",
      "-----------------------------------------------------\n",
      "783 M     Trainable params\n",
      "0         Non-trainable params\n",
      "783 M     Total params\n",
      "3,132.600 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63773e3734814b13aed9c2e5b3b264b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 2989: 'val_loss' reached 1.58178 (best 1.58178), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 5978: 'val_loss' reached 1.32776 (best 1.32776), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 8967: 'val_loss' reached 1.16963 (best 1.16963), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 11956: 'val_loss' reached 1.03473 (best 1.03473), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 14945: 'val_loss' reached 0.97187 (best 0.97187), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 17934: 'val_loss' reached 0.94873 (best 0.94873), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 20923: 'val_loss' reached 0.92274 (best 0.92274), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 23912: 'val_loss' reached 0.90186 (best 0.90186), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 26901: 'val_loss' reached 0.89086 (best 0.89086), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 29890: 'val_loss' reached 0.87498 (best 0.87498), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 32879: 'val_loss' reached 0.87007 (best 0.87007), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 35868: 'val_loss' reached 0.86251 (best 0.86251), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 38857: 'val_loss' reached 0.85703 (best 0.85703), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 41846: 'val_loss' reached 0.85261 (best 0.85261), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 44835: 'val_loss' reached 0.85228 (best 0.85228), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 47824: 'val_loss' reached 0.85154 (best 0.85154), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 50813: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 53802: 'val_loss' reached 0.84827 (best 0.84827), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_span_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 56791: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 59780: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 62769: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 65758: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 68747: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 71736: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0783cf08f646bf9fe8cb68cad0e11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 74725: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7dae2ad0-9e9f-4cb2-b142-c77cf0282a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c7a927398f40d89707786d018297d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.8776357769966125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.8776357769966125}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbcd7de1-16ff-414a-b5c8-1e4e29e3fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predictions\n",
    "finetuned_model = GSMModel.load_from_checkpoint(\"Finetuned_checkpoint/best_model_span_large.ckpt\")\n",
    "finetuned_model.freeze()\n",
    "#finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "701f0e84-cca3-469a-9579-3c4af98d4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, return_sequences):\n",
    "    source_encoding = tokenizer(\n",
    "        question[\"question\"],\n",
    "        question[\"context\"],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Move the input tensors to the same device as the model\n",
    "    input_ids = source_encoding[\"input_ids\"].to(finetuned_model.device)\n",
    "    attention_mask = source_encoding[\"attention_mask\"].to(finetuned_model.device)\n",
    "\n",
    "    generated_ids = finetuned_model.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        num_beams=120,\n",
    "        max_length=512,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "        repetition_penalty=3.0,\n",
    "        #temperature=1,\n",
    "        num_return_sequences = return_sequences\n",
    "    )\n",
    "\n",
    "    preds = [tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_space=True)\n",
    "             for gen_id in generated_ids]\n",
    "\n",
    "    return preds#\" \".join(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03bef66-40dd-4ecf-80eb-a34288882ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "finetuned_model = finetuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba36ff-601d-4f49-a693-c71b681ea749",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_outcome = {}\n",
    "\n",
    "# Generate sequences for the largest value in pass_list\n",
    "max_return_seq = 80 #max(pass_list)\n",
    "sequences = {}  # Dictionary to store the generated sequences for each question\n",
    "\n",
    "for i in range(586, len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    sequences[i] = generate_answer(question, max_return_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6a49b-e1f5-4597-8369-23eab54a2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f17a5-1803-456a-8735-b78cbfb25bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequences).to_csv('predictions_t5_large_jaccard_586+.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a364c4a7-9527-4517-9d86-e6a6f548dc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 586), (80, 733))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = pd.read_csv('predictions_t5_large_jaccard_586.csv')\n",
    "pred2 = pd.read_csv('predictions_t5_large_jaccard_586+.csv')\n",
    "pred1.shape, pred2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a4a583-1483-40ac-a9d8-5dc5cdb1c43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1319)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.concat([pred1, pred2], axis=1)\n",
    "df_combined.to_csv('predictions_t5_large_jaccard_temp1_penality3.csv', index=False)\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae40152a-8416-44c5-bb91-ddf23baa69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a dictionary\n",
    "df_combined_dict = df_combined.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c43a49e-a428-4f2a-a064-bdf3aa92d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Accuracy for pass@:1 is 0.12736921910538287\n",
      "Pass Accuracy for pass@:1 is 0.12736921910538287\n",
      "Absolute Accuracy for pass@:2 is 0.07657316148597422\n",
      "Pass Accuracy for pass@:2 is 0.15314632297194844\n",
      "Absolute Accuracy for pass@:3 is 0.058630275461207984\n",
      "Pass Accuracy for pass@:3 is 0.17589082638362397\n",
      "Absolute Accuracy for pass@:4 is 0.047763457164518575\n",
      "Pass Accuracy for pass@:4 is 0.1910538286580743\n",
      "Absolute Accuracy for pass@:5 is 0.0401819560272934\n",
      "Pass Accuracy for pass@:5 is 0.20090978013646701\n",
      "Absolute Accuracy for pass@:6 is 0.03538033864038413\n",
      "Pass Accuracy for pass@:6 is 0.21228203184230476\n",
      "Absolute Accuracy for pass@:7 is 0.032275533412758586\n",
      "Pass Accuracy for pass@:7 is 0.2259287338893101\n",
      "Absolute Accuracy for pass@:8 is 0.029757391963608795\n",
      "Pass Accuracy for pass@:8 is 0.23805913570887036\n",
      "Absolute Accuracy for pass@:9 is 0.02720916519248589\n",
      "Pass Accuracy for pass@:9 is 0.244882486732373\n",
      "Absolute Accuracy for pass@:10 is 0.02532221379833207\n",
      "Pass Accuracy for pass@:10 is 0.2532221379833207\n",
      "Absolute Accuracy for pass@:11 is 0.023709421738231443\n",
      "Pass Accuracy for pass@:11 is 0.2608036391205459\n",
      "Absolute Accuracy for pass@:12 is 0.022049532474096536\n",
      "Pass Accuracy for pass@:12 is 0.26459438968915844\n",
      "Absolute Accuracy for pass@:13 is 0.02105324546567913\n",
      "Pass Accuracy for pass@:13 is 0.27369219105382864\n",
      "Absolute Accuracy for pass@:14 is 0.019928517274991876\n",
      "Pass Accuracy for pass@:14 is 0.2789992418498863\n",
      "Absolute Accuracy for pass@:15 is 0.01910538286580743\n",
      "Pass Accuracy for pass@:15 is 0.28658074298711145\n",
      "Absolute Accuracy for pass@:16 is 0.01805344958301744\n",
      "Pass Accuracy for pass@:16 is 0.288855193328279\n",
      "Absolute Accuracy for pass@:17 is 0.017259064353565536\n",
      "Pass Accuracy for pass@:17 is 0.2934040940106141\n",
      "Absolute Accuracy for pass@:18 is 0.016426585797321203\n",
      "Pass Accuracy for pass@:18 is 0.29567854435178165\n",
      "Absolute Accuracy for pass@:19 is 0.01564183392522246\n",
      "Pass Accuracy for pass@:19 is 0.2971948445792267\n",
      "Absolute Accuracy for pass@:20 is 0.014973464746019712\n",
      "Pass Accuracy for pass@:20 is 0.29946929492039426\n",
      "Absolute Accuracy for pass@:21 is 0.014513159319831041\n",
      "Pass Accuracy for pass@:21 is 0.30477634571645185\n",
      "Absolute Accuracy for pass@:22 is 0.013991315735060997\n",
      "Pass Accuracy for pass@:22 is 0.3078089461713419\n",
      "Absolute Accuracy for pass@:23 is 0.013646702047005308\n",
      "Pass Accuracy for pass@:23 is 0.31387414708112205\n",
      "Absolute Accuracy for pass@:24 is 0.013236037402072277\n",
      "Pass Accuracy for pass@:24 is 0.31766489764973466\n",
      "Absolute Accuracy for pass@:25 is 0.01285822592873389\n",
      "Pass Accuracy for pass@:25 is 0.3214556482183472\n",
      "Absolute Accuracy for pass@:26 is 0.012509476876421531\n",
      "Pass Accuracy for pass@:26 is 0.3252463987869598\n",
      "Absolute Accuracy for pass@:27 is 0.012186561087243423\n",
      "Pass Accuracy for pass@:27 is 0.3290371493555724\n",
      "Absolute Accuracy for pass@:28 is 0.011913787501353839\n",
      "Pass Accuracy for pass@:28 is 0.33358605003790753\n",
      "Absolute Accuracy for pass@:29 is 0.011633682779535175\n",
      "Pass Accuracy for pass@:29 is 0.3373768006065201\n",
      "Absolute Accuracy for pass@:30 is 0.011296436694465504\n",
      "Pass Accuracy for pass@:30 is 0.3388931008339651\n",
      "Absolute Accuracy for pass@:31 is 0.01095649196605444\n",
      "Pass Accuracy for pass@:31 is 0.33965125094768767\n",
      "Absolute Accuracy for pass@:32 is 0.010803639120545869\n",
      "Pass Accuracy for pass@:32 is 0.3457164518574678\n",
      "Absolute Accuracy for pass@:33 is 0.010545178854504102\n",
      "Pass Accuracy for pass@:33 is 0.3479909021986353\n",
      "Absolute Accuracy for pass@:34 is 0.010301922133523614\n",
      "Pass Accuracy for pass@:34 is 0.3502653525398029\n",
      "Absolute Accuracy for pass@:35 is 0.010137550092061085\n",
      "Pass Accuracy for pass@:35 is 0.354814253222138\n",
      "Absolute Accuracy for pass@:36 is 0.009961250105298627\n",
      "Pass Accuracy for pass@:36 is 0.35860500379075055\n",
      "Absolute Accuracy for pass@:37 is 0.009794479847550356\n",
      "Pass Accuracy for pass@:37 is 0.36239575435936316\n",
      "Absolute Accuracy for pass@:38 is 0.009556681696660149\n",
      "Pass Accuracy for pass@:38 is 0.36315390447308565\n",
      "Absolute Accuracy for pass@:39 is 0.0094282770552672\n",
      "Pass Accuracy for pass@:39 is 0.36770280515542075\n",
      "Absolute Accuracy for pass@:40 is 0.009287338893100834\n",
      "Pass Accuracy for pass@:40 is 0.37149355572403336\n",
      "Absolute Accuracy for pass@:41 is 0.009116292830858559\n",
      "Pass Accuracy for pass@:41 is 0.3737680060652009\n",
      "Absolute Accuracy for pass@:42 is 0.00893534062601538\n",
      "Pass Accuracy for pass@:42 is 0.3752843062926459\n",
      "Absolute Accuracy for pass@:43 is 0.00878043620078636\n",
      "Pass Accuracy for pass@:43 is 0.3775587566338135\n",
      "Absolute Accuracy for pass@:44 is 0.008649803570197808\n",
      "Pass Accuracy for pass@:44 is 0.38059135708870356\n",
      "Absolute Accuracy for pass@:45 is 0.00849128127369219\n",
      "Pass Accuracy for pass@:45 is 0.3821076573161486\n",
      "Absolute Accuracy for pass@:46 is 0.008372614299370406\n",
      "Pass Accuracy for pass@:46 is 0.38514025777103866\n",
      "Absolute Accuracy for pass@:47 is 0.008210604423080026\n",
      "Pass Accuracy for pass@:47 is 0.3858984078847612\n",
      "Absolute Accuracy for pass@:48 is 0.008055344958301744\n",
      "Pass Accuracy for pass@:48 is 0.3866565579984837\n",
      "Absolute Accuracy for pass@:49 is 0.00793736751713574\n",
      "Pass Accuracy for pass@:49 is 0.3889310083396513\n",
      "Absolute Accuracy for pass@:50 is 0.007869598180439728\n",
      "Pass Accuracy for pass@:50 is 0.3934799090219864\n",
      "Absolute Accuracy for pass@:51 is 0.007774755087781891\n",
      "Pass Accuracy for pass@:51 is 0.39651250947687644\n",
      "Absolute Accuracy for pass@:52 is 0.0076835598063801245\n",
      "Pass Accuracy for pass@:52 is 0.3995451099317665\n",
      "Absolute Accuracy for pass@:53 is 0.00761011057547885\n",
      "Pass Accuracy for pass@:53 is 0.40333586050037906\n",
      "Absolute Accuracy for pass@:54 is 0.007525341869542021\n",
      "Pass Accuracy for pass@:54 is 0.40636846095526913\n",
      "Absolute Accuracy for pass@:55 is 0.00741608656695844\n",
      "Pass Accuracy for pass@:55 is 0.40788476118271416\n",
      "Absolute Accuracy for pass@:56 is 0.007310733239467129\n",
      "Pass Accuracy for pass@:56 is 0.4094010614101592\n",
      "Absolute Accuracy for pass@:57 is 0.007182474761581741\n",
      "Pass Accuracy for pass@:57 is 0.4094010614101592\n",
      "Absolute Accuracy for pass@:58 is 0.007084782097200073\n",
      "Pass Accuracy for pass@:58 is 0.41091736163760423\n",
      "Absolute Accuracy for pass@:59 is 0.006977551046632657\n",
      "Pass Accuracy for pass@:59 is 0.4116755117513268\n",
      "Absolute Accuracy for pass@:60 is 0.006911801870103614\n",
      "Pass Accuracy for pass@:60 is 0.41470811220621684\n",
      "Absolute Accuracy for pass@:61 is 0.00681092233311376\n",
      "Pass Accuracy for pass@:61 is 0.41546626231993933\n",
      "Absolute Accuracy for pass@:62 is 0.006749981657658539\n",
      "Pass Accuracy for pass@:62 is 0.4184988627748294\n",
      "Absolute Accuracy for pass@:63 is 0.006642839091663959\n",
      "Pass Accuracy for pass@:63 is 0.4184988627748294\n",
      "Absolute Accuracy for pass@:64 is 0.006539044730856709\n",
      "Pass Accuracy for pass@:64 is 0.4184988627748294\n",
      "Absolute Accuracy for pass@:65 is 0.00646177173849653\n",
      "Pass Accuracy for pass@:65 is 0.4200151630022744\n",
      "Absolute Accuracy for pass@:66 is 0.006398327474900637\n",
      "Pass Accuracy for pass@:66 is 0.422289613343442\n",
      "Absolute Accuracy for pass@:67 is 0.006325461396580403\n",
      "Pass Accuracy for pass@:67 is 0.42380591357088704\n",
      "Absolute Accuracy for pass@:68 is 0.006277036970967311\n",
      "Pass Accuracy for pass@:68 is 0.4268385140257771\n",
      "Absolute Accuracy for pass@:69 is 0.0061860654206634365\n",
      "Pass Accuracy for pass@:69 is 0.4268385140257771\n",
      "Absolute Accuracy for pass@:70 is 0.006141015921152388\n",
      "Pass Accuracy for pass@:70 is 0.42987111448066717\n",
      "Absolute Accuracy for pass@:71 is 0.006086557251011757\n",
      "Pass Accuracy for pass@:71 is 0.43214556482183475\n",
      "Absolute Accuracy for pass@:72 is 0.006033611321708365\n",
      "Pass Accuracy for pass@:72 is 0.43442001516300227\n",
      "Absolute Accuracy for pass@:73 is 0.005961344729818148\n",
      "Pass Accuracy for pass@:73 is 0.4351781652767248\n",
      "Absolute Accuracy for pass@:74 is 0.005880786017253037\n",
      "Pass Accuracy for pass@:74 is 0.4351781652767248\n",
      "Absolute Accuracy for pass@:75 is 0.005832701541571898\n",
      "Pass Accuracy for pass@:75 is 0.43745261561789234\n",
      "Absolute Accuracy for pass@:76 is 0.005765931128047564\n",
      "Pass Accuracy for pass@:76 is 0.4382107657316149\n",
      "Absolute Accuracy for pass@:77 is 0.005710741116351427\n",
      "Pass Accuracy for pass@:77 is 0.4397270659590599\n",
      "Absolute Accuracy for pass@:78 is 0.005695845726171731\n",
      "Pass Accuracy for pass@:78 is 0.444275966641395\n",
      "Absolute Accuracy for pass@:79 is 0.0056525369238299055\n",
      "Pass Accuracy for pass@:79 is 0.44655041698256254\n",
      "Absolute Accuracy for pass@:80 is 0.005581880212282032\n",
      "Pass Accuracy for pass@:80 is 0.44655041698256254\n"
     ]
    }
   ],
   "source": [
    "pass_outcome = {}\n",
    "max_return_seq = 80\n",
    "\n",
    "for return_seq in range(1,max_return_seq+1):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "        predictions = df_combined_dict[str(i)][:return_seq]  # Use the pre-generated sequences up to return_seq\n",
    "        question = test_data.iloc[i]\n",
    "        answer = question.answer\n",
    "        ground_truth = question.answer.split('#### ')[-1]\n",
    "        for generated_seq in predictions:\n",
    "            # Extract the final answer from the generated sequence\n",
    "            generated_answer = generated_seq.split('#### ')[-1]\n",
    "            if generated_answer == ground_truth:\n",
    "                correct_predictions += 1\n",
    "                break\n",
    "    pass_outcome[return_seq] = correct_predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / (return_seq * len(test_data))\n",
    "    print(f'Absolute Accuracy for pass@:{return_seq} is {accuracy}')\n",
    "    pass_accuracy = correct_predictions / len(test_data)\n",
    "    print(f'Pass Accuracy for pass@:{return_seq} is {pass_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8898dcb-9dea-4ed0-88a1-7e8854eaaa95",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 47.54 GiB total capacity; 39.33 GiB already allocated; 9.31 MiB free; 46.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_data)):\n\u001b[1;32m      8\u001b[0m     question \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m----> 9\u001b[0m     sequences[i] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_return_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence generation complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Iterate over pass_list and calculate metrics using the generated sequences\u001b[39;00m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(question, return_sequences)\u001b[0m\n\u001b[1;32m     14\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m source_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(finetuned_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m source_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(finetuned_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 17\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#temperature=1,\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m preds \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(gen_id, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m gen_id \u001b[38;5;129;01min\u001b[39;00m generated_ids]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1053\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;66;03m# interleave with `num_beams`\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1051\u001b[0m         input_ids, expand_size\u001b[38;5;241m=\u001b[39mnum_beams, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1052\u001b[0m     )\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1067\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1068\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k, top_p\u001b[38;5;241m=\u001b[39mtop_p, temperature\u001b[38;5;241m=\u001b[39mtemperature, num_beams\u001b[38;5;241m=\u001b[39mnum_beams\n\u001b[1;32m   1069\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1857\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   1854\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   1855\u001b[0m )\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1857\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# increase cur_len\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1715\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration._reorder_cache\u001b[0;34m(self, past, beam_idx)\u001b[0m\n\u001b[1;32m   1711\u001b[0m reordered_layer_past_states \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past_state \u001b[38;5;129;01min\u001b[39;00m layer_past_states:\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;66;03m# need to set correct `past` for each of the four key / value states\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m     reordered_layer_past_states \u001b[38;5;241m=\u001b[39m reordered_layer_past_states \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m-> 1715\u001b[0m         \u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m reordered_layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reordered_layer_past_states) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(layer_past_states)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 47.54 GiB total capacity; 39.33 GiB already allocated; 9.31 MiB free; 46.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "pass_outcome = {}\n",
    "\n",
    "# Generate sequences for the largest value in pass_list\n",
    "max_return_seq = 50 #max(pass_list)\n",
    "sequences = {}  # Dictionary to store the generated sequences for each question\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    sequences[i] = generate_answer(question, max_return_seq)\n",
    "\n",
    "print(\"sequence generation complete\")\n",
    "# Iterate over pass_list and calculate metrics using the generated sequences\n",
    "for return_seq in range(1,max_return_seq+1):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "\n",
    "        predictions = sequences[i][:return_seq]  # Use the pre-generated sequences up to return_seq\n",
    "        question = test_data.iloc[i]\n",
    "        answer = question.answer\n",
    "        ground_truth = question.answer.split('#### ')[-1]\n",
    "        \n",
    "        for generated_seq in predictions:\n",
    "            # Extract the final answer from the generated sequence\n",
    "            generated_answer = generated_seq.split('#### ')[-1]\n",
    "            if generated_answer == ground_truth:\n",
    "                correct_predictions += 1\n",
    "                break\n",
    "            \n",
    "    pass_outcome[\"pass@\"+str(return_seq)] = correct_predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / (return_seq * len(test_data))\n",
    "    print(f'Absolute Accuracy for pass@:{return_seq} is {accuracy}')\n",
    "    pass_accuracy = correct_predictions / len(test_data)\n",
    "    print(f'Pass Accuracy for pass@:{return_seq} is {pass_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb1d48-56b8-4f1d-9683-296bbe86f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequences).to_csv('predictions_t5_large_jaccard.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f69d081c-60a8-4079-a365-de49043d8f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f26d32-bcfe-4c55-9867-8c64bc3a3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequences).to_csv('predictions_t5_large_jaccard.csv', index=False)  # Save the test_data with predictions as a CSV file\n",
    "\n",
    "import csv\n",
    "filename = 'pass_outcome_t5_large_jaccard.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pass', 'Outcome'])\n",
    "    for key, value in pass_outcome.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57d60bd-0881-45ce-8a71-46da0e93e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "filename = 'pass_outcome_t5_large_jaccard_temp1_penality3.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pass', 'Outcome'])\n",
    "    for key, value in pass_outcome.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a4d09-5877-4429-8627-77fa570aaeab",
   "metadata": {},
   "source": [
    "## Below is the pass sequence computation when we change the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f9c088-e512-4b60-9be3-19652a8aee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, return_sequences, temp):\n",
    "    source_encoding = tokenizer(\n",
    "        question[\"question\"],\n",
    "        question[\"context\"],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Move the input tensors to the same device as the model\n",
    "    input_ids = source_encoding[\"input_ids\"].to(finetuned_model.device)\n",
    "    attention_mask = source_encoding[\"attention_mask\"].to(finetuned_model.device)\n",
    "\n",
    "    generated_ids = finetuned_model.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        num_beams=80,\n",
    "        max_length=512,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "        repetition_penalty=3.0,\n",
    "        temperature=temp,\n",
    "        num_return_sequences = return_sequences\n",
    "    )\n",
    "\n",
    "    preds = [tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_space=True)\n",
    "             for gen_id in generated_ids]\n",
    "\n",
    "    return preds#\" \".join(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60137cc-9fd9-4ab9-abae-e69f7c08dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_outcome = {}\n",
    "\n",
    "# Generate sequences for the largest value in pass_list\n",
    "max_return_seq = 50 \n",
    "sequences = {}  # Dictionary to store the generated sequences for each question\n",
    "temp = 1\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    sequences[i] = generate_answer(question, max_return_seq, temp)\n",
    "\n",
    "print(\"sequence generation complete\")\n",
    "# Iterate over pass_list and calculate metrics using the generated sequences\n",
    "for return_seq in range(1,max_return_seq+1):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "        predictions = sequences[i][:return_seq]  # Use the pre-generated sequences up to return_seq\n",
    "        question = test_data.iloc[i]\n",
    "        answer = question.answer\n",
    "        ground_truth = question.answer.split('#### ')[-1]\n",
    "        for generated_seq in predictions:\n",
    "            # Extract the final answer from the generated sequence\n",
    "            generated_answer = generated_seq.split('#### ')[-1]\n",
    "            if generated_answer == ground_truth:\n",
    "                correct_predictions += 1\n",
    "                break\n",
    "    pass_outcome[return_seq] = correct_predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    pass_accuracy = correct_predictions / len(test_data)\n",
    "    print(f'Pass Accuracy for pass@:{return_seq} is {pass_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eea57a-07b6-4187-ab76-a66bc3a9cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequences).to_csv('predictions_t5_large_jc_temp1_penality_3.csv', index=False)  # Save the test_data with predictions as a CSV file\n",
    "import csv\n",
    "filename = 'pass_outcome_t5_large_jc_temp1_penality_3.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pass', 'Outcome'])\n",
    "    for key, value in pass_outcome.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b6318-6ca1-4b4c-9704-a792defb9a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence generation complete\n",
      "Pass Accuracy for pass@:1 is 0.12054586808188021\n",
      "Pass Accuracy for pass@:2 is 0.14935557240333586\n",
      "Pass Accuracy for pass@:3 is 0.16982562547384383\n",
      "Pass Accuracy for pass@:4 is 0.19029567854435178\n",
      "Pass Accuracy for pass@:5 is 0.20318423047763456\n",
      "Pass Accuracy for pass@:6 is 0.2137983320697498\n",
      "Pass Accuracy for pass@:7 is 0.22365428354814254\n",
      "Pass Accuracy for pass@:8 is 0.2304776345716452\n",
      "Pass Accuracy for pass@:9 is 0.24184988627748294\n",
      "Pass Accuracy for pass@:10 is 0.2532221379833207\n",
      "Pass Accuracy for pass@:11 is 0.2630780894617134\n",
      "Pass Accuracy for pass@:12 is 0.2676269901440485\n",
      "Pass Accuracy for pass@:13 is 0.2721758908263836\n",
      "Pass Accuracy for pass@:14 is 0.27748294162244125\n",
      "Pass Accuracy for pass@:15 is 0.27824109173616374\n",
      "Pass Accuracy for pass@:16 is 0.2835481425322214\n",
      "Pass Accuracy for pass@:17 is 0.29037149355572406\n",
      "Pass Accuracy for pass@:18 is 0.2941622441243366\n",
      "Pass Accuracy for pass@:19 is 0.2979529946929492\n",
      "Pass Accuracy for pass@:20 is 0.29946929492039426\n",
      "Pass Accuracy for pass@:21 is 0.3025018953752843\n",
      "Pass Accuracy for pass@:22 is 0.30477634571645185\n",
      "Pass Accuracy for pass@:23 is 0.3070507960576194\n",
      "Pass Accuracy for pass@:24 is 0.310841546626232\n",
      "Pass Accuracy for pass@:25 is 0.3169067475360121\n",
      "Pass Accuracy for pass@:26 is 0.3199393479909022\n",
      "Pass Accuracy for pass@:27 is 0.3260045489006823\n",
      "Pass Accuracy for pass@:28 is 0.33206974981046244\n",
      "Pass Accuracy for pass@:29 is 0.33510235026535257\n",
      "Pass Accuracy for pass@:30 is 0.3388931008339651\n",
      "Pass Accuracy for pass@:31 is 0.34268385140257773\n",
      "Pass Accuracy for pass@:32 is 0.34723275208491283\n",
      "Pass Accuracy for pass@:33 is 0.34950720242608035\n",
      "Pass Accuracy for pass@:34 is 0.35329795299469297\n",
      "Pass Accuracy for pass@:35 is 0.3570887035633055\n",
      "Pass Accuracy for pass@:36 is 0.36087945413191813\n",
      "Pass Accuracy for pass@:37 is 0.3646702047005307\n",
      "Pass Accuracy for pass@:38 is 0.36770280515542075\n",
      "Pass Accuracy for pass@:39 is 0.36997725549658833\n",
      "Pass Accuracy for pass@:40 is 0.37452615617892343\n",
      "Pass Accuracy for pass@:41 is 0.37680060652009095\n",
      "Pass Accuracy for pass@:42 is 0.379833206974981\n",
      "Pass Accuracy for pass@:43 is 0.38286580742987114\n",
      "Pass Accuracy for pass@:44 is 0.38362395754359363\n",
      "Pass Accuracy for pass@:45 is 0.3866565579984837\n",
      "Pass Accuracy for pass@:46 is 0.3889310083396513\n",
      "Pass Accuracy for pass@:47 is 0.3904473085670963\n",
      "Pass Accuracy for pass@:48 is 0.39272175890826383\n",
      "Pass Accuracy for pass@:49 is 0.39423805913570886\n",
      "Pass Accuracy for pass@:50 is 0.39651250947687644\n"
     ]
    }
   ],
   "source": [
    "pass_outcome = {}\n",
    "\n",
    "# Generate sequences for the largest value in pass_list\n",
    "max_return_seq = 50 #max(pass_list)\n",
    "sequences = {}  # Dictionary to store the generated sequences for each question\n",
    "temp = 0.9\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    sequences[i] = generate_answer(question, max_return_seq, temp)\n",
    "\n",
    "print(\"sequence generation complete\")\n",
    "# Iterate over pass_list and calculate metrics using the generated sequences\n",
    "for return_seq in range(1,max_return_seq+1):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "        predictions = sequences[i][:return_seq]  # Use the pre-generated sequences up to return_seq\n",
    "        question = test_data.iloc[i]\n",
    "        answer = question.answer\n",
    "        ground_truth = question.answer.split('#### ')[-1]\n",
    "        for generated_seq in predictions:\n",
    "            # Extract the final answer from the generated sequence\n",
    "            generated_answer = generated_seq.split('#### ')[-1]\n",
    "            if generated_answer == ground_truth:\n",
    "                correct_predictions += 1\n",
    "                break\n",
    "    pass_outcome[return_seq] = correct_predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    pass_accuracy = correct_predictions / len(test_data)\n",
    "    print(f'Pass Accuracy for pass@:{return_seq} is {pass_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64635890-102a-4dc9-b5d6-930d1908d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequences).to_csv('predictions_t5_large_jc_temp90.csv', index=False)  # Save the test_data with predictions as a CSV file\n",
    "import csv\n",
    "filename = 'pass_outcome_t5_large_jc_temp90.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pass', 'Outcome'])\n",
    "    for key, value in pass_outcome.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c2ac6-dcda-4c1b-937c-470f509e93ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence generation complete\n",
      "Absolute Accuracy for pass@:1 is 0.12054586808188021\n",
      "Pass Accuracy for pass@:1 is 0.12054586808188021\n",
      "Absolute Accuracy for pass@:2 is 0.07467778620166793\n",
      "Pass Accuracy for pass@:2 is 0.14935557240333586\n",
      "Absolute Accuracy for pass@:3 is 0.056608541824614604\n",
      "Pass Accuracy for pass@:3 is 0.16982562547384383\n",
      "Absolute Accuracy for pass@:4 is 0.047573919636087945\n",
      "Pass Accuracy for pass@:4 is 0.19029567854435178\n",
      "Absolute Accuracy for pass@:5 is 0.04063684609552692\n",
      "Pass Accuracy for pass@:5 is 0.20318423047763456\n",
      "Absolute Accuracy for pass@:6 is 0.0356330553449583\n",
      "Pass Accuracy for pass@:6 is 0.2137983320697498\n",
      "Absolute Accuracy for pass@:7 is 0.03195061193544893\n",
      "Pass Accuracy for pass@:7 is 0.22365428354814254\n",
      "Absolute Accuracy for pass@:8 is 0.02880970432145565\n",
      "Pass Accuracy for pass@:8 is 0.2304776345716452\n",
      "Absolute Accuracy for pass@:9 is 0.026872209586386992\n",
      "Pass Accuracy for pass@:9 is 0.24184988627748294\n",
      "Absolute Accuracy for pass@:10 is 0.02532221379833207\n",
      "Pass Accuracy for pass@:10 is 0.2532221379833207\n",
      "Absolute Accuracy for pass@:11 is 0.023916189951064856\n",
      "Pass Accuracy for pass@:11 is 0.2630780894617134\n",
      "Absolute Accuracy for pass@:12 is 0.02230224917867071\n",
      "Pass Accuracy for pass@:12 is 0.2676269901440485\n",
      "Absolute Accuracy for pass@:13 is 0.020936606986644894\n",
      "Pass Accuracy for pass@:13 is 0.2721758908263836\n",
      "Absolute Accuracy for pass@:14 is 0.01982021011588866\n",
      "Pass Accuracy for pass@:14 is 0.27748294162244125\n",
      "Absolute Accuracy for pass@:15 is 0.018549406115744252\n",
      "Pass Accuracy for pass@:15 is 0.27824109173616374\n",
      "Absolute Accuracy for pass@:16 is 0.017721758908263836\n",
      "Pass Accuracy for pass@:16 is 0.2835481425322214\n",
      "Absolute Accuracy for pass@:17 is 0.01708067609151318\n",
      "Pass Accuracy for pass@:17 is 0.29037149355572406\n",
      "Absolute Accuracy for pass@:18 is 0.01634234689579648\n",
      "Pass Accuracy for pass@:18 is 0.2941622441243366\n",
      "Absolute Accuracy for pass@:19 is 0.0156817365627868\n",
      "Pass Accuracy for pass@:19 is 0.2979529946929492\n",
      "Absolute Accuracy for pass@:20 is 0.014973464746019712\n",
      "Pass Accuracy for pass@:20 is 0.29946929492039426\n",
      "Absolute Accuracy for pass@:21 is 0.014404852160727824\n",
      "Pass Accuracy for pass@:21 is 0.3025018953752843\n",
      "Absolute Accuracy for pass@:22 is 0.013853470259838721\n",
      "Pass Accuracy for pass@:22 is 0.30477634571645185\n",
      "Absolute Accuracy for pass@:23 is 0.013350034611200844\n",
      "Pass Accuracy for pass@:23 is 0.3070507960576194\n",
      "Absolute Accuracy for pass@:24 is 0.012951731109426334\n",
      "Pass Accuracy for pass@:24 is 0.310841546626232\n",
      "Absolute Accuracy for pass@:25 is 0.012676269901440485\n",
      "Pass Accuracy for pass@:25 is 0.3169067475360121\n",
      "Absolute Accuracy for pass@:26 is 0.012305359538111623\n",
      "Pass Accuracy for pass@:26 is 0.3199393479909022\n",
      "Absolute Accuracy for pass@:27 is 0.012074242551877123\n",
      "Pass Accuracy for pass@:27 is 0.3260045489006823\n",
      "Absolute Accuracy for pass@:28 is 0.011859633921802231\n",
      "Pass Accuracy for pass@:28 is 0.33206974981046244\n",
      "Absolute Accuracy for pass@:29 is 0.01155525345742595\n",
      "Pass Accuracy for pass@:29 is 0.33510235026535257\n",
      "Absolute Accuracy for pass@:30 is 0.011296436694465504\n",
      "Pass Accuracy for pass@:30 is 0.3388931008339651\n",
      "Absolute Accuracy for pass@:31 is 0.011054317787179926\n",
      "Pass Accuracy for pass@:31 is 0.34268385140257773\n",
      "Absolute Accuracy for pass@:32 is 0.010851023502653526\n",
      "Pass Accuracy for pass@:32 is 0.34723275208491283\n",
      "Absolute Accuracy for pass@:33 is 0.01059112734624486\n",
      "Pass Accuracy for pass@:33 is 0.34950720242608035\n",
      "Absolute Accuracy for pass@:34 is 0.010391116264549792\n",
      "Pass Accuracy for pass@:34 is 0.35329795299469297\n",
      "Absolute Accuracy for pass@:35 is 0.010202534387523015\n",
      "Pass Accuracy for pass@:35 is 0.3570887035633055\n",
      "Absolute Accuracy for pass@:36 is 0.01002442928144217\n",
      "Pass Accuracy for pass@:36 is 0.36087945413191813\n",
      "Absolute Accuracy for pass@:37 is 0.009855951478392721\n",
      "Pass Accuracy for pass@:37 is 0.3646702047005307\n",
      "Absolute Accuracy for pass@:38 is 0.009676389609353178\n",
      "Pass Accuracy for pass@:38 is 0.36770280515542075\n",
      "Absolute Accuracy for pass@:39 is 0.009486596294784317\n",
      "Pass Accuracy for pass@:39 is 0.36997725549658833\n",
      "Absolute Accuracy for pass@:40 is 0.009363153904473086\n",
      "Pass Accuracy for pass@:40 is 0.37452615617892343\n",
      "Absolute Accuracy for pass@:41 is 0.009190258695611976\n",
      "Pass Accuracy for pass@:41 is 0.37680060652009095\n",
      "Absolute Accuracy for pass@:42 is 0.009043647785118597\n",
      "Pass Accuracy for pass@:42 is 0.379833206974981\n",
      "Absolute Accuracy for pass@:43 is 0.00890385598674119\n",
      "Pass Accuracy for pass@:43 is 0.38286580742987114\n",
      "Absolute Accuracy for pass@:44 is 0.008718726307808946\n",
      "Pass Accuracy for pass@:44 is 0.38362395754359363\n",
      "Absolute Accuracy for pass@:45 is 0.00859236795552186\n",
      "Pass Accuracy for pass@:45 is 0.3866565579984837\n",
      "Absolute Accuracy for pass@:46 is 0.008455021920427202\n",
      "Pass Accuracy for pass@:46 is 0.3889310083396513\n",
      "Absolute Accuracy for pass@:47 is 0.008307389543980772\n",
      "Pass Accuracy for pass@:47 is 0.3904473085670963\n",
      "Absolute Accuracy for pass@:48 is 0.00818170331058883\n",
      "Pass Accuracy for pass@:48 is 0.39272175890826383\n",
      "Absolute Accuracy for pass@:49 is 0.008045674676238956\n",
      "Pass Accuracy for pass@:49 is 0.39423805913570886\n",
      "Absolute Accuracy for pass@:50 is 0.007930250189537528\n",
      "Pass Accuracy for pass@:50 is 0.39651250947687644\n"
     ]
    }
   ],
   "source": [
    "pass_outcome = {}\n",
    "\n",
    "# Generate sequences for the largest value in pass_list\n",
    "max_return_seq = 50 #max(pass_list)\n",
    "sequences = {}  # Dictionary to store the generated sequences for each question\n",
    "temp = 0.85\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    sequences[i] = generate_answer(question, max_return_seq, temp)\n",
    "\n",
    "print(\"sequence generation complete\")\n",
    "# Iterate over pass_list and calculate metrics using the generated sequences\n",
    "for return_seq in range(1,max_return_seq+1):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "        predictions = sequences[i][:return_seq]  # Use the pre-generated sequences up to return_seq\n",
    "        question = test_data.iloc[i]\n",
    "        answer = question.answer\n",
    "        ground_truth = question.answer.split('#### ')[-1]\n",
    "        for generated_seq in predictions:\n",
    "            # Extract the final answer from the generated sequence\n",
    "            generated_answer = generated_seq.split('#### ')[-1]\n",
    "            if generated_answer == ground_truth:\n",
    "                correct_predictions += 1\n",
    "                break\n",
    "    pass_outcome[return_seq] = correct_predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / (return_seq * len(test_data))\n",
    "    print(f'Absolute Accuracy for pass@:{return_seq} is {accuracy}')\n",
    "    pass_accuracy = correct_predictions / len(test_data)\n",
    "    print(f'Pass Accuracy for pass@:{return_seq} is {pass_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282ac23-e458-4c89-919e-a764a2f2ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequences).to_csv('predictions_t5_large_jc_temp85.csv', index=False)  # Save the test_data with predictions as a CSV file\n",
    "import csv\n",
    "filename = 'pass_outcome_t5_large_jc_temp85.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pass', 'Outcome'])\n",
    "    for key, value in pass_outcome.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248155b-fd33-4bb3-93d8-71c52b690134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence generation complete\n",
      "Absolute Accuracy for pass@:1 is 0.12054586808188021\n",
      "Pass Accuracy for pass@:1 is 0.12054586808188021\n",
      "Absolute Accuracy for pass@:2 is 0.07467778620166793\n",
      "Pass Accuracy for pass@:2 is 0.14935557240333586\n",
      "Absolute Accuracy for pass@:3 is 0.056608541824614604\n",
      "Pass Accuracy for pass@:3 is 0.16982562547384383\n",
      "Absolute Accuracy for pass@:4 is 0.047573919636087945\n",
      "Pass Accuracy for pass@:4 is 0.19029567854435178\n",
      "Absolute Accuracy for pass@:5 is 0.04063684609552692\n",
      "Pass Accuracy for pass@:5 is 0.20318423047763456\n",
      "Absolute Accuracy for pass@:6 is 0.0356330553449583\n",
      "Pass Accuracy for pass@:6 is 0.2137983320697498\n",
      "Absolute Accuracy for pass@:7 is 0.03195061193544893\n",
      "Pass Accuracy for pass@:7 is 0.22365428354814254\n",
      "Absolute Accuracy for pass@:8 is 0.02880970432145565\n",
      "Pass Accuracy for pass@:8 is 0.2304776345716452\n",
      "Absolute Accuracy for pass@:9 is 0.026872209586386992\n",
      "Pass Accuracy for pass@:9 is 0.24184988627748294\n",
      "Absolute Accuracy for pass@:10 is 0.02532221379833207\n",
      "Pass Accuracy for pass@:10 is 0.2532221379833207\n",
      "Absolute Accuracy for pass@:11 is 0.023916189951064856\n",
      "Pass Accuracy for pass@:11 is 0.2630780894617134\n",
      "Absolute Accuracy for pass@:12 is 0.02230224917867071\n",
      "Pass Accuracy for pass@:12 is 0.2676269901440485\n",
      "Absolute Accuracy for pass@:13 is 0.020936606986644894\n",
      "Pass Accuracy for pass@:13 is 0.2721758908263836\n",
      "Absolute Accuracy for pass@:14 is 0.01982021011588866\n",
      "Pass Accuracy for pass@:14 is 0.27748294162244125\n",
      "Absolute Accuracy for pass@:15 is 0.018549406115744252\n",
      "Pass Accuracy for pass@:15 is 0.27824109173616374\n",
      "Absolute Accuracy for pass@:16 is 0.017721758908263836\n",
      "Pass Accuracy for pass@:16 is 0.2835481425322214\n",
      "Absolute Accuracy for pass@:17 is 0.01708067609151318\n",
      "Pass Accuracy for pass@:17 is 0.29037149355572406\n",
      "Absolute Accuracy for pass@:18 is 0.01634234689579648\n",
      "Pass Accuracy for pass@:18 is 0.2941622441243366\n",
      "Absolute Accuracy for pass@:19 is 0.0156817365627868\n",
      "Pass Accuracy for pass@:19 is 0.2979529946929492\n",
      "Absolute Accuracy for pass@:20 is 0.014973464746019712\n",
      "Pass Accuracy for pass@:20 is 0.29946929492039426\n",
      "Absolute Accuracy for pass@:21 is 0.014404852160727824\n",
      "Pass Accuracy for pass@:21 is 0.3025018953752843\n",
      "Absolute Accuracy for pass@:22 is 0.013853470259838721\n",
      "Pass Accuracy for pass@:22 is 0.30477634571645185\n",
      "Absolute Accuracy for pass@:23 is 0.013350034611200844\n",
      "Pass Accuracy for pass@:23 is 0.3070507960576194\n",
      "Absolute Accuracy for pass@:24 is 0.012951731109426334\n",
      "Pass Accuracy for pass@:24 is 0.310841546626232\n",
      "Absolute Accuracy for pass@:25 is 0.012676269901440485\n",
      "Pass Accuracy for pass@:25 is 0.3169067475360121\n",
      "Absolute Accuracy for pass@:26 is 0.012305359538111623\n",
      "Pass Accuracy for pass@:26 is 0.3199393479909022\n",
      "Absolute Accuracy for pass@:27 is 0.012074242551877123\n",
      "Pass Accuracy for pass@:27 is 0.3260045489006823\n",
      "Absolute Accuracy for pass@:28 is 0.011859633921802231\n",
      "Pass Accuracy for pass@:28 is 0.33206974981046244\n",
      "Absolute Accuracy for pass@:29 is 0.01155525345742595\n",
      "Pass Accuracy for pass@:29 is 0.33510235026535257\n",
      "Absolute Accuracy for pass@:30 is 0.011296436694465504\n",
      "Pass Accuracy for pass@:30 is 0.3388931008339651\n",
      "Absolute Accuracy for pass@:31 is 0.011054317787179926\n",
      "Pass Accuracy for pass@:31 is 0.34268385140257773\n",
      "Absolute Accuracy for pass@:32 is 0.010851023502653526\n",
      "Pass Accuracy for pass@:32 is 0.34723275208491283\n",
      "Absolute Accuracy for pass@:33 is 0.01059112734624486\n",
      "Pass Accuracy for pass@:33 is 0.34950720242608035\n",
      "Absolute Accuracy for pass@:34 is 0.010391116264549792\n",
      "Pass Accuracy for pass@:34 is 0.35329795299469297\n",
      "Absolute Accuracy for pass@:35 is 0.010202534387523015\n",
      "Pass Accuracy for pass@:35 is 0.3570887035633055\n",
      "Absolute Accuracy for pass@:36 is 0.01002442928144217\n",
      "Pass Accuracy for pass@:36 is 0.36087945413191813\n",
      "Absolute Accuracy for pass@:37 is 0.009855951478392721\n",
      "Pass Accuracy for pass@:37 is 0.3646702047005307\n",
      "Absolute Accuracy for pass@:38 is 0.009676389609353178\n",
      "Pass Accuracy for pass@:38 is 0.36770280515542075\n",
      "Absolute Accuracy for pass@:39 is 0.009486596294784317\n",
      "Pass Accuracy for pass@:39 is 0.36997725549658833\n",
      "Absolute Accuracy for pass@:40 is 0.009363153904473086\n",
      "Pass Accuracy for pass@:40 is 0.37452615617892343\n",
      "Absolute Accuracy for pass@:41 is 0.009190258695611976\n",
      "Pass Accuracy for pass@:41 is 0.37680060652009095\n",
      "Absolute Accuracy for pass@:42 is 0.009043647785118597\n",
      "Pass Accuracy for pass@:42 is 0.379833206974981\n",
      "Absolute Accuracy for pass@:43 is 0.00890385598674119\n",
      "Pass Accuracy for pass@:43 is 0.38286580742987114\n",
      "Absolute Accuracy for pass@:44 is 0.008718726307808946\n",
      "Pass Accuracy for pass@:44 is 0.38362395754359363\n",
      "Absolute Accuracy for pass@:45 is 0.00859236795552186\n",
      "Pass Accuracy for pass@:45 is 0.3866565579984837\n",
      "Absolute Accuracy for pass@:46 is 0.008455021920427202\n",
      "Pass Accuracy for pass@:46 is 0.3889310083396513\n",
      "Absolute Accuracy for pass@:47 is 0.008307389543980772\n",
      "Pass Accuracy for pass@:47 is 0.3904473085670963\n",
      "Absolute Accuracy for pass@:48 is 0.00818170331058883\n",
      "Pass Accuracy for pass@:48 is 0.39272175890826383\n",
      "Absolute Accuracy for pass@:49 is 0.008045674676238956\n",
      "Pass Accuracy for pass@:49 is 0.39423805913570886\n",
      "Absolute Accuracy for pass@:50 is 0.007930250189537528\n",
      "Pass Accuracy for pass@:50 is 0.39651250947687644\n"
     ]
    }
   ],
   "source": [
    "pass_outcome = {}\n",
    "\n",
    "# Generate sequences for the largest value in pass_list\n",
    "max_return_seq = 50 #max(pass_list)\n",
    "sequences = {}  # Dictionary to store the generated sequences for each question\n",
    "temp = 0.8\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    sequences[i] = generate_answer(question, max_return_seq, temp)\n",
    "\n",
    "print(\"sequence generation complete\")\n",
    "# Iterate over pass_list and calculate metrics using the generated sequences\n",
    "for return_seq in range(1,max_return_seq+1):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "        predictions = sequences[i][:return_seq]  # Use the pre-generated sequences up to return_seq\n",
    "        question = test_data.iloc[i]\n",
    "        answer = question.answer\n",
    "        ground_truth = question.answer.split('#### ')[-1]\n",
    "        for generated_seq in predictions:\n",
    "            # Extract the final answer from the generated sequence\n",
    "            generated_answer = generated_seq.split('#### ')[-1]\n",
    "            if generated_answer == ground_truth:\n",
    "                correct_predictions += 1\n",
    "                break\n",
    "    pass_outcome[return_seq] = correct_predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / (return_seq * len(test_data))\n",
    "    print(f'Absolute Accuracy for pass@:{return_seq} is {accuracy}')\n",
    "    pass_accuracy = correct_predictions / len(test_data)\n",
    "    print(f'Pass Accuracy for pass@:{return_seq} is {pass_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7cd45f-8c66-40c9-876d-c60e26082bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequences).to_csv('predictions_t5_large_jc_temp80.csv', index=False)  # Save the test_data with predictions as a CSV file\n",
    "import csv\n",
    "filename = 'pass_outcome_t5_large_jc_temp80.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pass', 'Outcome'])\n",
    "    for key, value in pass_outcome.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19ea5c-4713-4ff6-93e5-22931ede0eb0",
   "metadata": {},
   "source": [
    "## Temperature iterations finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcfc6bbd-743b-47c9-8e5a-ce0d1d9eea93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context     Hannah needs to drink 100 ml of water for ever...\n",
       "question         How many ml of water does she need to drink?\n",
       "answer      First find the total calories burned on aerobi...\n",
       "Name: 586, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = test_data.iloc[586]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe39692f-a311-4323-a22a-d1169287f07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First find the total calories burned on aerobics: 2 hours * 500 calories/hour = <<2*500=1000>>1000 calories\\nThen add the calories burned running to find the total calories burned: 600 calories + 1000 calories = <<600+1000=1600>>1600 calories\\nFinally, divide that number by the ratio of calories burned to water drunk to find how many water Hannah needs to drink: 1600 calories * 100 ml/200 calories = <<1600*100/200=800>>800 ml\\n#### 800'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "531f4d8f-9d5e-44e3-8dee-bbb67524dfa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 47.54 GiB total capacity; 39.33 GiB already allocated; 9.31 MiB free; 46.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m answer\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(question, return_sequences)\u001b[0m\n\u001b[1;32m     14\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m source_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(finetuned_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m source_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(finetuned_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 17\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#temperature=1,\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m preds \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(gen_id, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m gen_id \u001b[38;5;129;01min\u001b[39;00m generated_ids]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1053\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;66;03m# interleave with `num_beams`\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1051\u001b[0m         input_ids, expand_size\u001b[38;5;241m=\u001b[39mnum_beams, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1052\u001b[0m     )\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1067\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1068\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k, top_p\u001b[38;5;241m=\u001b[39mtop_p, temperature\u001b[38;5;241m=\u001b[39mtemperature, num_beams\u001b[38;5;241m=\u001b[39mnum_beams\n\u001b[1;32m   1069\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1857\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   1854\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   1855\u001b[0m )\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1857\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# increase cur_len\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1715\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration._reorder_cache\u001b[0;34m(self, past, beam_idx)\u001b[0m\n\u001b[1;32m   1711\u001b[0m reordered_layer_past_states \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past_state \u001b[38;5;129;01min\u001b[39;00m layer_past_states:\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;66;03m# need to set correct `past` for each of the four key / value states\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m     reordered_layer_past_states \u001b[38;5;241m=\u001b[39m reordered_layer_past_states \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m-> 1715\u001b[0m         \u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m reordered_layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reordered_layer_past_states) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(layer_past_states)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 47.54 GiB total capacity; 39.33 GiB already allocated; 9.31 MiB free; 46.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "answer = generate_answer(question, 1)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaf357e9-040c-4642-ac81-c84330edb70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "\n",
    "for generated_seq in answer:\n",
    "    # Extract the final answer from the generated sequence\n",
    "    generated_answer = generated_seq.split('#### ')[-1]\n",
    "    actual_answer = question.answer.split('#### ')[-1]\n",
    "    # Compare the generated answer with the ground truth\n",
    "    if generated_answer == actual_answer:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / len(answer)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "974e30d0-677b-4d10-a159-9f3a45f2fa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b87d5c3-88aa-40f0-a175-c0dfdba0fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.02031842304776346\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "predictions_list = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    answer = question.answer\n",
    "    \n",
    "    ground_truth = question.answer.split('#### ')[-1]\n",
    "    predictions = generate_answer(question)\n",
    "    predictions_list.append(predictions)\n",
    "    for generated_seq in predictions:\n",
    "        # Extract the final answer from the generated sequence\n",
    "        generated_answer = generated_seq.split('#### ')[-1]\n",
    "        if generated_answer == actual_answer:\n",
    "            correct_predictions += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / (5 * len(test_data))\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89d4826b-af79-4cf3-9f3e-7b529a9e4b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14233c3-b56f-41a3-85cc-f15d8e8aadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finetuning on the Large T5 model to set the baseline\n",
    "pretrained_model_path2 = 'google/t5-v1_1-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34717cdd-93c1-46b9-be6e-65bfae97faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_model_path2)\n",
    "model = T5ForConditionalGeneration.from_pretrained(pretrained_model_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "468378e6-b85a-466a-a7a7-701cff29d07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68e3a61f606456297af85e8e2c6577f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420217c7a81a49168f00a82e63c83b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4184a340a584b1499c7412620eb650a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebb25326b964350b2c10dec80780b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f57ed4b3b0e4dd89f3ff6bd53ed83d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m pretrained_model_path2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/t5-v1_1-large\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_path2)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_path2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mT5_large\u001b[39;00m(pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:1297\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m hf_bucket_url(\n\u001b[1;32m   1289\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   1290\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1291\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1292\u001b[0m         mirror\u001b[38;5;241m=\u001b[39mmirror,\n\u001b[1;32m   1293\u001b[0m     )\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m-> 1297\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43marchive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1308\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(err)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py:1402\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     local_files_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m-> 1402\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py:1666\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m   1664\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1666\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1668\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1669\u001b[0m os\u001b[38;5;241m.\u001b[39mreplace(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py:1525\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1515\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m   1517\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1518\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[1;32m   1524\u001b[0m )\n\u001b[0;32m-> 1525\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m   1527\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 760\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    761\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/response.py:575\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 575\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/urllib3/response.py:518\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     cache_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    520\u001b[0m         amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\n\u001b[1;32m    521\u001b[0m     ):  \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class T5_large(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(pretrained_model_path2, return_dict = True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "model2 = T5_large()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b230a8-956a-42d8-819d-dbf307da3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"Finetuned_checkpoint\",\n",
    "    filename='best_model_large',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cc2b4-c6d0-4e72-ba21-dddc79150b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs = 25,\n",
    "    #logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ce4eade-ea10-428f-b27d-91bd16a5c31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/work/GSM8K/CustomT5/lightning_logs\n",
      "/home/work/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/work/GSM8K/CustomT5/Finetuned_checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 783 M \n",
      "-----------------------------------------------------\n",
      "783 M     Trainable params\n",
      "0         Non-trainable params\n",
      "783 M     Total params\n",
      "3,132.600 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900a958b209449ee8a5116022073e4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 2989: 'val_loss' reached 4.39661 (best 4.39661), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 5978: 'val_loss' reached 1.85001 (best 1.85001), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 8967: 'val_loss' reached 1.46090 (best 1.46090), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 11956: 'val_loss' reached 1.28854 (best 1.28854), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 14945: 'val_loss' reached 1.18146 (best 1.18146), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 17934: 'val_loss' reached 1.10508 (best 1.10508), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 20923: 'val_loss' reached 1.05347 (best 1.05347), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 23912: 'val_loss' reached 1.01184 (best 1.01184), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 26901: 'val_loss' reached 0.98198 (best 0.98198), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 29890: 'val_loss' reached 0.95629 (best 0.95629), saving model to '/home/work/GSM8K/CustomT5/Finetuned_checkpoint/best_model_large.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model2,data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbdc7112-08ba-4cfe-b41e-3ba17a4f6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predictions\n",
    "finetuned_model2 = T5_large.load_from_checkpoint(\"Finetuned_checkpoint/best_model_large.ckpt\")\n",
    "finetuned_model2.freeze()\n",
    "\n",
    "def generate_answer(question, return_sequences):\n",
    "    source_encoding = tokenizer(\n",
    "        question[\"question\"],\n",
    "        question[\"context\"],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Move the input tensors to the same device as the model\n",
    "    input_ids = source_encoding[\"input_ids\"].to(finetuned_model.device)\n",
    "    attention_mask = source_encoding[\"attention_mask\"].to(finetuned_model.device)\n",
    "\n",
    "    generated_ids = finetuned_model2.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        num_beams=20,\n",
    "        max_length=512,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "        repetition_penalty=2.5,\n",
    "        #temperature=1,\n",
    "        num_return_sequences = return_sequences\n",
    "    )\n",
    "\n",
    "    preds = [tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_space=True)\n",
    "             for gen_id in generated_ids]\n",
    "\n",
    "    return preds#\" \".join(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2571993-d8f4-4750-8039-54c71bf3d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "predictions_list2 = []\n",
    "pass_outcome = {}\n",
    "\n",
    "# Generate sequences for the largest value in pass_list\n",
    "max_return_seq = 1 #max(pass_list)\n",
    "sequences = {}  # Dictionary to store the generated sequences for each question\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    question = test_data.iloc[i]\n",
    "    answer = question.answer\n",
    "    ground_truth = question.answer.split('#### ')[-1]\n",
    "    sequences[i] = generate_answer(question, max_return_seq)\n",
    "\n",
    "# Iterate over pass_list and calculate metrics using the generated sequences\n",
    "for return_seq in range(1,51):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "        predictions = sequences[i][:return_seq]  # Use the pre-generated sequences up to return_seq\n",
    "        for generated_seq in predictions:\n",
    "            # Extract the final answer from the generated sequence\n",
    "            generated_answer = generated_seq.split('#### ')[-1]\n",
    "            if generated_answer == ground_truth:\n",
    "                correct_predictions += 1\n",
    "\n",
    "    pass_outcome[return_seq] = correct_predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / (return_seq * len(test_data))\n",
    "    print(f'Absolute Accuracy for pass@:{return_seq} is {accuracy}')\n",
    "    pass_accuracy = correct_predictions / len(test_data)\n",
    "    print(f'Pass Accuracy for pass@:{return_seq} is {pass_accuracy}')\n",
    "\n",
    "sequences.to_csv('predictions_t5_large.csv', index=False)  # Save the test_data with predictions as a CSV file\n",
    "pass_outcome.to_csv('pass_outcomes_t5_large.csv', index=False)  # Save the pass_outcome as a CSV file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.13 (NGC 22.05/Python 3.8 Conda) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
